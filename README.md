# Personal AI Agent System

> A self-improving, privacy-focused AI agent orchestrator that runs on local hardware with optional cloud model fallback

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

An autonomous AI agent system designed for personal use that:

- **Orchestrates specialized agents** for different task types (code, research, file operations)
- **Runs primarily on local models** (Ollama) to minimize API costs and maximize privacy
- **Sandboxes all file operations** to prevent access to personal data
- **Self-improves with human approval** - proposes new skills and optimizations
- **Scales from modest hardware** (NAS) to powerful local inference (Mac Mini M4 Pro)

**Philosophy**: Privacy-first, cost-conscious, extensible, and transparent. You control what the agent can access and approve all self-modifications.

---

## Quick Start

### Prerequisites

- Python 3.11 or higher
- Ollama installed (for local models)
- Optional: Anthropic API key (for cloud fallback)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/personal-ai-agent.git
cd personal-ai-agent

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install and start Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull qwen3:1.7b  # Fast classification model
ollama pull qwen3:8b    # General purpose model

# Run the agent
python agent.py --interactive
```

### First Run

```bash
# Test with a simple task
python agent.py "Write a haiku about coding"

# Start interactive mode
python agent.py -i

# View statistics
python agent.py stats
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACE                            â”‚
â”‚  Terminal CLI â”‚ Web Dashboard â”‚ API Gateway â”‚ Cron Scheduler â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   ORCHESTRATOR      â”‚ â† Routes tasks to agents
          â”‚   LiteLLM Router    â”‚ â† Manages model selection
          â”‚   Persistent State  â”‚ â† Tracks history
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Code Agent   â”‚ â”‚Research â”‚ â”‚FileOps Agentâ”‚
â”‚Writes skills â”‚ â”‚ Agent   â”‚ â”‚Sandboxed FS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            SANDBOXED WORKSPACE              â”‚
â”‚  /skills  â”‚  /memory  â”‚  /temp  â”‚ /outputs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components**:

1. **Orchestrator**: Main routing brain, spawns specialized agents
2. **Agents**: Ephemeral workers with specific skills (code, research, file ops)
3. **Skill Library**: Reusable Python/JavaScript/Shell scripts created by agents
4. **Memory System**: SQLite + vector database for task history and learning
5. **Approval Queue**: Human-in-the-loop for self-modifications
6. **Model Router**: LiteLLM gateway supporting local (Ollama) and cloud (Claude, GPT)

---

## Features

### âœ… Current (Phase 0-2)

- [x] Basic orchestrator with task routing
- [x] Claude API integration with tier-based routing
- [x] SQLite task history and statistics
- [x] Interactive CLI interface
- [x] Cost tracking per model

### ğŸš§ In Progress (Phase 3-4)

- [ ] Ollama local model integration
- [ ] Specialized agent profiles (Code, Research, FileOps)
- [ ] Sandboxed filesystem with approval gates
- [ ] Skill library system
- [ ] Web dashboard for approval queue

### ğŸ”® Planned (Phase 5-6)

- [ ] Self-improvement cron jobs
- [ ] Vector memory (ChromaDB)
- [ ] RAG over past tasks
- [ ] Multi-agent collaboration
- [ ] Proactive skill optimization

---

## Project Structure

```
personal-ai-agent/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ SPEC.md                   # Technical specifications
â”œâ”€â”€ DEVELOPMENT.md            # Development guide
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package configuration
â”‚
â”œâ”€â”€ agent/                    # Main application code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ orchestrator.py      # Main orchestrator logic
â”‚   â”œâ”€â”€ router.py            # Model routing (LiteLLM)
â”‚   â”œâ”€â”€ database.py          # SQLite task storage
â”‚   â”œâ”€â”€ sandbox.py           # File sandboxing
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/              # Specialized agent implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py          # Base agent class
â”‚   â”‚   â”œâ”€â”€ code_agent.py    # Code generation agent
â”‚   â”‚   â”œâ”€â”€ research_agent.py # Web research agent
â”‚   â”‚   â””â”€â”€ fileops_agent.py # File operations agent
â”‚   â”‚
â”‚   â”œâ”€â”€ memory/              # Memory and learning systems
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ task_history.py  # SQLite task logging
â”‚   â”‚   â”œâ”€â”€ vector_store.py  # ChromaDB semantic memory
â”‚   â”‚   â””â”€â”€ skill_registry.py # Skill management
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/               # Tool implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ web_search.py
â”‚   â”‚   â”œâ”€â”€ file_tools.py
â”‚   â”‚   â””â”€â”€ code_exec.py
â”‚   â”‚
â”‚   â””â”€â”€ api/                 # Web API (FastAPI)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py           # FastAPI application
â”‚       â”œâ”€â”€ routes/
â”‚       â””â”€â”€ models/
â”‚
â”œâ”€â”€ config/                   # Configuration files
â”‚   â”œâ”€â”€ litellm.yaml         # Model routing config
â”‚   â”œâ”€â”€ agents/              # Agent profile definitions
â”‚   â”‚   â”œâ”€â”€ code_agent.yaml
â”‚   â”‚   â”œâ”€â”€ research_agent.yaml
â”‚   â”‚   â””â”€â”€ fileops_agent.yaml
â”‚   â””â”€â”€ tools.yaml           # Tool configurations
â”‚
â”œâ”€â”€ workspace/               # Sandboxed agent workspace
â”‚   â”œâ”€â”€ skills/              # Agent-created skills
â”‚   â”œâ”€â”€ memory/              # Persistent memory files
â”‚   â”œâ”€â”€ temp/                # Temporary scratch space
â”‚   â””â”€â”€ outputs/             # User-facing outputs
â”‚
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ setup_ollama.sh     # Install and configure Ollama
â”‚   â”œâ”€â”€ migrate_db.py       # Database migrations
â”‚   â””â”€â”€ benchmark.py        # Performance testing
â”‚
â””â”€â”€ docs/                    # Documentation
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ api.md
    â”œâ”€â”€ agents.md
    â””â”€â”€ skills.md
```

---

## Configuration

### Model Configuration (`config/litellm.yaml`)

```yaml
model_list:
  # Local models (free, private)
  - model_name: local-fast
    litellm_params:
      model: ollama/qwen3:1.7b
      api_base: http://localhost:11434
      
  - model_name: local-smart
    litellm_params:
      model: ollama/qwen3:8b
      api_base: http://localhost:11434
  
  # Cloud fallback (optional)
  - model_name: cloud-smart
    litellm_params:
      model: claude-sonnet-4-20250514
      api_key: ${ANTHROPIC_API_KEY}

router_settings:
  routing_strategy: "usage-based-routing"
  fallback_models:
    - local-smart
    - cloud-smart
```

### Agent Configuration (`config/agents/code_agent.yaml`)

```yaml
name: CodeAgent
description: Writes Python/JavaScript/Shell skills for the system
model_tier: smart  # Uses local-smart or cloud-smart
max_iterations: 3
approval_required: true

tools:
  - file_write
  - bash_execution
  - skill_validator

system_prompt: |
  You are a code generation specialist. Write clean, tested, 
  documented skills that follow best practices.
  
  Rules:
  1. Include SKILL.md header with usage examples
  2. Add error handling and input validation
  3. Write deterministic, testable code
  4. Never access files outside /skills directory
```

---

## Usage Examples

### CLI Commands

```bash
# Single task execution
python agent.py "Summarize this article: https://example.com/article"

# Force specific model tier
python agent.py --tier smart "Design a database schema for a blog"

# Interactive mode
python agent.py --interactive

# View statistics
python agent.py stats --days 30

# View recent history
python agent.py history --limit 20

# Export task history
python agent.py export --format json --output tasks.json
```

### Python API

```python
from agent import Orchestrator, TaskDatabase

# Initialize
db = TaskDatabase("workspace/memory/tasks.db")
orchestrator = Orchestrator(db=db)

# Execute task
result = await orchestrator.execute_task(
    user_input="Write a Python script to parse CSV files",
    force_tier="smart"
)

print(result.output)
print(f"Used model: {result.model}")
print(f"Tokens: {result.tokens}")
```

### Web API (Future)

```bash
# Start web server
python -m agent.api

# API endpoints
POST   /api/tasks              # Execute task
GET    /api/tasks/{task_id}    # Get task result
GET    /api/approvals          # List pending approvals
POST   /api/approvals/{id}     # Approve/reject proposal
GET    /api/stats              # Usage statistics
```

---

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=agent --cov-report=html

# Run specific test file
pytest tests/unit/test_orchestrator.py

# Run integration tests
pytest tests/integration/ -v
```

### Code Quality

```bash
# Linting
ruff check agent/
black --check agent/

# Type checking
mypy agent/

# Format code
black agent/
ruff check --fix agent/
```

### Adding a New Agent

1. Create agent profile in `config/agents/your_agent.yaml`
2. Implement agent class in `agent/agents/your_agent.py`
3. Register in `agent/agents/__init__.py`
4. Add tests in `tests/unit/test_your_agent.py`
5. Update documentation

See [DEVELOPMENT.md](DEVELOPMENT.md) for detailed guidelines.

---

## Performance

### Model Performance (Intel i3-10100F NAS, CPU-only)

| Model | Size | Tokens/sec | Use Case |
|-------|------|------------|----------|
| Qwen3-1.7B (Q4) | 1.2GB | 15-25 | Fast classification |
| Qwen3-8B (Q4) | 5GB | 3-8 | General reasoning |
| Llama 3.2-3B (Q4) | 2GB | 10-20 | Summarization |

### Model Performance (Mac Mini M4 Pro, 48GB RAM)

| Model | Size | Tokens/sec | Use Case |
|-------|------|------------|----------|
| Qwen3-30B-A3B (Q4) | 18GB | 40-50 | Primary reasoning |
| Qwen3-8B (Q4) | 5GB | 60-90 | Fast tasks |
| gpt-oss-20b (Q4) | 16GB | 45-60 | Complex analysis |

### Cost Comparison

| Configuration | Monthly Cost | Quality | Speed |
|---------------|--------------|---------|-------|
| 100% Local (NAS) | $0 | Medium | Slow |
| 100% Local (Mac Mini) | $0 | High | Fast |
| Hybrid (Local + Haiku) | $5-15 | High | Fast |
| 100% Cloud (Sonnet) | $50-150 | Highest | Fast |

---

## Security & Privacy

### File Sandboxing

All file operations are restricted to the workspace directory:

```
workspace/
â”œâ”€â”€ skills/      # Agent can write (with approval)
â”œâ”€â”€ memory/      # Agent can read/write
â”œâ”€â”€ temp/        # Agent can read/write
â””â”€â”€ outputs/     # Agent can write

/home/user/      # Agent CANNOT access
```

### Approval Gates

Operations requiring human approval:
- Creating new skills
- Modifying agent configurations
- Writing to output directory
- Installing packages
- Running shell commands (optional)

### Data Privacy

- **All personal files isolated** - agent never sees them
- **Local-first architecture** - data stays on your hardware
- **Optional cloud routing** - you control when/if data goes to APIs
- **Audit logging** - all actions logged to SQLite

---

## Roadmap

### Phase 1: Foundation âœ…
- [x] Basic orchestrator
- [x] Claude API integration
- [x] Task history database

### Phase 2: Local Models ğŸš§
- [ ] Ollama integration
- [ ] LiteLLM routing
- [ ] Cost optimization

### Phase 3: Agents & Sandboxing ğŸ“‹
- [ ] Specialized agent profiles
- [ ] File sandboxing
- [ ] Skill library system

### Phase 4: Self-Improvement ğŸ“‹
- [ ] Cron-based analysis
- [ ] Skill proposals
- [ ] Approval queue UI

### Phase 5: Memory & Learning ğŸ“‹
- [ ] Vector database
- [ ] RAG over task history
- [ ] Pattern recognition

### Phase 6: Production Ready ğŸ“‹
- [ ] Web dashboard
- [ ] Multi-agent orchestration
- [ ] Performance monitoring

---

## Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

### Development Setup

```bash
# Fork and clone
git clone https://github.com/yourusername/personal-ai-agent.git
cd personal-ai-agent

# Install in development mode
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run tests
pytest
```

---

## License

MIT License - see [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- Inspired by [OpenClaw](https://openclaw.ai/) architecture
- Built on [LiteLLM](https://github.com/BerriAI/litellm) for model routing
- Uses [Ollama](https://ollama.ai/) for local inference
- Powered by [Qwen](https://github.com/QwenLM) models

---

## Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/yourusername/personal-ai-agent/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/personal-ai-agent/discussions)

---

## FAQ

**Q: Do I need an API key?**  
A: No! The system is designed to run 100% locally with Ollama. API keys are optional for cloud fallback.

**Q: What hardware do I need?**  
A: Minimum: 8GB RAM, modern CPU. Recommended: 16GB+ RAM, or Mac with M-series chip for best performance.

**Q: How much does it cost to run?**  
A: $0 if you run locally. Optional cloud API usage: $5-20/month for light use.

**Q: Is my data safe?**  
A: Yes. All file operations are sandboxed. The agent cannot access your personal files unless you explicitly move them into the workspace.

**Q: Can it modify itself?**  
A: Only with your approval. All self-modifications go to an approval queue that you review via web UI or CLI.

**Q: How is this different from OpenClaw?**  
A: We add: file sandboxing, approval gates, orchestrator pattern, and cost optimization. OpenClaw is more powerful but less safe.
